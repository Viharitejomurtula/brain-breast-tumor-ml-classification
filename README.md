# üß† Multi-Modal Tumor Classification with Explainable AI

<div align="center">

**Brain + Breast MRI Classification | Custom TrigConv2D Architecture | GRAD-CAM & Integrated Gradients**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://tensorflow.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[View Demo Notebook](notebooks/02_explainability_demo.ipynb) ‚Ä¢ [Explainability Results](#-explainability-deep-dive) ‚Ä¢ [Contact](#-contact)

</div>

---

## üéØ Project Summary

> **Not just classification ‚Äî understanding *why* the model decides what it decides.**

This project builds a **6-class tumor classifier** across two MRI modalities (brain & breast) with a focus on **explainable AI**. The key insight: different attribution methods reveal different aspects of model reasoning.

| What I Built | Why It Matters |
|--------------|----------------|
| Multi-modal CNN classifier | Handles anatomically different scan types in one model |
| Custom `TrigConv2D` layer | Novel feature extraction using sin/cos frequency kernels |
| Dual explainability pipeline | GRAD-CAM + Integrated Gradients reveal complementary insights |
| Low-confidence analysis | Shows model uncertainty correlates with diffuse explanations |

---

## ‚ö° Key Results

<table>
<tr>
<td width="50%">

### üìä Model Performance
| Metric | Value |
|--------|-------|
| **Test Accuracy** | 95.95% |
| **Classes** | 6 |
| **Input Size** | 128 √ó 128 √ó 3 |
| **Total Test Samples** | 4,739 |

</td>
<td width="50%">

### üî¨ Explainability Insight
> *"GRAD-CAM captures where the model looks to classify **modality**. Integrated Gradients reveals where the **pathology** is."*

This distinction is critical for clinical interpretability.

</td>
</tr>
</table>

---

## üî¨ Explainability Deep Dive

### The Core Discovery

The model learns a **two-step decision hierarchy**:

```
Step 1: Modality Recognition    ‚Üí    "Is this a brain or breast scan?"
Step 2: Tumor Classification    ‚Üí    "What type of tumor (if any)?"
```

**Different explainability methods expose different steps:**

| Method | What It Reveals | Resolution | Best For |
|--------|-----------------|------------|----------|
| **GRAD-CAM** | Broad anatomical attention zones | Coarse (feature map) | Verifying modality focus |
| **Integrated Gradients** | Pixel-level tumor attribution | Fine (input resolution) | Clinical interpretability |

---

### üì∏ Case Study Visualizations

#### Case 1: High-Confidence Breast MRI (Benign)
| Original | GRAD-CAM | Integrated Gradients | Prediction |
|----------|----------|---------------------|------------|
| ![Original](assets/case1_original.png) | ![GRAD-CAM](assets/case1_gradcam.png) | ![IG](assets/case1_ig.png) | ‚úÖ Benign (100.0%) |

> **Interpretation:** GRAD-CAM highlights breast tissue boundaries (modality). IG isolates the lesion core (pathology).

---

#### Case 2: Brain MRI (Glioma Tumor)
| Original | GRAD-CAM | Integrated Gradients | Prediction |
|----------|----------|---------------------|------------|
| ![Original](assets/case2_original.png) | ![GRAD-CAM](assets/case2_gradcam.png) | ![IG](assets/case2_ig.png) | ‚úÖ Glioma (100.0%) |

> **Interpretation:** GRAD-CAM spreads across brain structure. IG pinpoints hyperintense tumor regions aligned with radiologist attention.

---

#### Case 3: Human Perception Bias
| Original | GRAD-CAM | Integrated Gradients | Prediction |
|----------|----------|---------------------|------------|
| ![Original](assets/case3_original.png) | ![GRAD-CAM](assets/case3_gradcam.png) | ![IG](assets/case3_ig.png) | ‚úÖ No Tumor (100.0%) |

> **Interpretation:** Even when humans might confuse the scan for another body part, the explainability maps show the model is still reasoning like a ‚Äúbrain detector".
---

#### Case 4: Low Confidence Prediction
| Original | GRAD-CAM | Integrated Gradients | Prediction |
|----------|----------|---------------------|------------|
| ![Original](assets/case3_original.png) | ![GRAD-CAM](assets/case3_gradcam.png) | ![IG](assets/case3_ig.png) | ‚ö†Ô∏è No Tumor (57.4%) |

> **Interpretation:** When confidence drops, both explanations become diffuse. This correlation between uncertainty and unfocused attribution indicates the model isn't hallucinating ‚Äî it's appropriately uncertain.

## üèóÔ∏è Architecture

### Custom TrigConv2D Layer

Instead of random initialization, the first convolutional layer uses **fixed trigonometric kernels**:

```python
# Even filters: sin(frequency √ó (x + y))
# Odd filters:  cos(frequency √ó (x + y))
```

**Why?** This encodes structured spatial frequency information from the start ‚Äî similar to positional encoding in transformers, but for images.

```
Input (128√ó128√ó3)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TrigConv2D    ‚îÇ  ‚Üê Sin/Cos frequency kernels (no learned weights)
‚îÇ   16 filters    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Conv2D       ‚îÇ  ‚Üê Standard learned convolution
‚îÇ   32 filters    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    MaxPooling
         ‚Üì
    Dense(64)
         ‚Üì
    Dense(6)  ‚Üí Softmax
```

---

## üìÅ Repository Structure

```
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 02_explainability_demo.ipynb   ‚≠ê START HERE - Full walkthrough
‚îÇ   ‚îî‚îÄ‚îÄ public_visualization.ipynb      Additional visualizations
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ model_trigconv2d.py            TrigConv2D layer definition
‚îÇ   ‚îî‚îÄ‚îÄ explainability.py              GRAD-CAM & IG implementations
‚îÇ
‚îú‚îÄ‚îÄ artifacts/
‚îÇ   ‚îú‚îÄ‚îÄ X_test_sample.npy              Test images (4,739 samples)
‚îÇ   ‚îú‚îÄ‚îÄ y_test_sample.npy              Test labels
‚îÇ   ‚îú‚îÄ‚îÄ label_names.npy                Class name mapping
‚îÇ   ‚îî‚îÄ‚îÄ trigconv_model.keras           Trained model weights
‚îÇ
‚îî‚îÄ‚îÄ assets/                            README images
```

---

## üöÄ Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/brain-breast-tumor-ml-classification.git
cd brain-breast-tumor-ml-classification

# Install dependencies
pip install tensorflow numpy matplotlib

# Run the explainability demo
jupyter notebook notebooks/02_explainability_demo.ipynb
```

**No training required** ‚Äî all artifacts are pre-computed and included.

---

## üìä Dataset

### Classes & Distribution

| Class | Modality | Description |
|-------|----------|-------------|
| **Benign** | Breast MRI | Non-cancerous breast lesion |
| **Malignant** | Breast MRI | Cancerous breast tumor |
| **No Tumor** | Brain MRI | Healthy brain scan |
| **Glioma Tumor** | Brain MRI | Tumor from glial cells |
| **Meningioma Tumor** | Brain MRI | Tumor from meninges |
| **Pituitary Tumor** | Brain MRI | Tumor in pituitary gland |

### Preprocessing Pipeline
- ‚úÖ BGR ‚Üí RGB conversion
- ‚úÖ Resize to 128 √ó 128
- ‚úÖ Normalize to [0, 1]
- ‚úÖ Stratified train/test split (before oversampling)
- ‚úÖ Class balancing via oversampling

---

## üìö Method Comparison

| Aspect | GRAD-CAM | Integrated Gradients |
|--------|----------|---------------------|
| **Computation** | Fast (single backward pass) | Slower (100 interpolation steps) |
| **Resolution** | Coarse (feature map size) | Fine (pixel-level) |
| **What it shows** | Regional activation | Pixel importance scores |
| **In this model** | Modality discrimination | Tumor-specific features |
| **Clinical use** | Verify anatomical focus | Identify diagnostic regions |

---

## üîÆ Future Work

- [ ] Adversarial robustness testing
- [ ] Confidence calibration curves
- [ ] Additional XAI methods (SHAP, LIME)
- [ ] Deployment as web application

---

## üìñ Citations

**Brain MRI Dataset:**
> Sartaj Bhuvaji, Ankita Kadam, Prajakta Bhumkar, Sameer Dedge, Swati Kanchan. (2020). Brain Tumor Classification (MRI). Kaggle. DOI: 10.34740/KAGGLE/DSV/1183165

**Breast MRI Dataset:**
> Breast MRI dataset from Kaggle medical imaging collection.

---

## üîí Code Privacy Note

The complete implementation (training scripts, data pipelines, full `TrigConv2D` implementation) is maintained in a **private repository** for academic integrity.

This public repository provides:
- ‚úÖ Trained model artifacts
- ‚úÖ Explainability demonstrations
- ‚úÖ Reproducible inference notebooks
- ‚úÖ Architecture documentation

**Recruiters & reviewers:** Full codebase available upon request.

---

## üì¨ Contact

**Vihari Tejo**

üìß [vihari5tejo@gmail.com](mailto:vihari5tejo@gmail.com)

üíº [LinkedIn](https://linkedin.com/in/yourprofile) ‚Ä¢ üêô [GitHub](https://github.com/yourusername)

---

<div align="center">

**‚≠ê If this project demonstrates the skills you're looking for, let's connect! ‚≠ê**

</div>
